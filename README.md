# Electronic Music Genre Recognition
## Electronic Subgenre Recognition Using Convolutional Neural Networks
### Abstract
The presented work proposes a new method of classifying music genres. The field of music genre recognition (MGR) aims to develop a system that can predict the genre of any song. However, the past 20 years of research on this topic have taken certain variables for granted, such as the size of the datasets and the genres used for classification. Calling these variables into question could discover valuable directions for the entire field. Since music genres are structured like a taxonomy, one could train a model on the subgenres of the genres used for testing by abstracting the predictions back to their main genre. This allows the model to learn about the specific ways a genre is manifested. Electronic music is a suitable testing ground for such an inquiry, since it is saturated with taxonomical subdivisions, the resulting classifier is relevant in contemporary society, and yet insufficient research has been conducted on this genre. No prior research was found using a neural network to classify electronic subgenres. Two convolutional neural network (CNN) models and two different datasets have been applied to test the effect of subgenre targeting. It achieved moderately positive results, only being beneficial for larger models and larger datasets. The best accuracy was 92.4\% on a binary classification task using no subgenre targeting. Future work could evaluate the effects of subgenre targeting on larger models and datasets while controlling for the amount of parameters.  
### Dataset Taxonomy
![The genre taxonomy](https://github.com/joramwessels/MGR/blob/master/taxonomy.png)

### Models
#### Small CNN
![The small CNN](https://github.com/joramwessels/MGR/blob/master/CNN.png)
#### k2c2
![The k2c2 proposed by (Choi, 2016)](https://github.com/joramwessels/MGR/blob/master/k2c2.png)

### Conclusion
The targeting of subgenres during training achieved only moderate improvements in classification accuracy compared to targeting the main genres. Its effects appear to be solely visible when using the small CNN on a large dataset. There might be two conflicting phenomena at work that explain this observation. Firstly, what subgenre targeting intends to do is to capture more information about a genre, namely the characteristics of its subgenres. Getting to know each genre better might require more data before it can contribute to the accuracy. Secondly, under the surface, subgenre targeting sabotages its own trianing process by having to attribute more features to multiple classes. The weights associated with those features that are central to a genre are essentially shared across the subgenres, lowering their influence in the network. This would improve overall accuracy when tested on the same abstraction, but the ability to differentiate between different subgenres within the same genre does not justify the loss of association between the weights and the main genre. This second hypothetical would have more influence the more subgenres there are relative to the amount of main genres. Compared to the first dataset, which has two subgenres per main genre, the second dataset contains 9 subgenres for 4 main genres. This extra singleton did not outweight the increase in dataset size, supposedly resulting in better subgenre targeting results for the second dataset.  
Contrary to initial assumptions, the small CNN does seem to outperform its big brother. Although it was intended as a k2c2 cut short by two layers, it appears that the multilayer perceptron at the end has had a positive effect on classification results. However, when counting the amount of parameters for each model, the multilayer perceptron appears to have a lot more than the last two layers of the k2c2, which explains the observations. The MLP maps 41 12x20 activation maps to 1024 hidden nodes, and those hidden nodes to the output layer, which amounts to $41*12*20*1024 + 1024*n\approx 10^7$. The last two convolutional layers have 62 and 83 3x3 kernels, and map 83 nodes to the output layer, resulting in $3*3*41*62 + 3*3*62*83 + 83*n\approx 69*10^3$. Coming back to the subgenre targeting, the additional specifics on the subgenres require more parameters to harbor that information. The amount of parameters in the 'small' CNN can explain its superiority. The 92.4\% accuracy achieved by this model is a token of potential, but binary classification is a long way from the typical 10 class classification.  
Although the two datasets can be compared in terms of classification accuracy, the cause of that difference can be attributed to either the number of classes or the total amount of samples. Because there was no control group for either of these variables, it is troublesome to draw any definite conclusions about them. However, what has become clear is that the first dataset achieved better results than the second. The increase in size did not outweigh the increase in classes, despite the increased average amount of samples per class. Therefore, either the ratio of samples per genre should be higher, or the typical use of small datasets in MGR is justified.
